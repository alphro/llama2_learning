{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77224726-d884-4690-95bd-d01fc1cd2444",
   "metadata": {},
   "source": [
    "# Presentation of LLama2\n",
    "In the following notebook, I will present learnings and code that display how to use and finetune using the Llama2 model. This will include various resources ranging from the Llama2 Documentation to youtube videos and tutorials which will be linked and credited."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a60aa467-8b50-44d0-907a-bc2769a03633",
   "metadata": {},
   "source": [
    "## What are Transformers?\n",
    "\n",
    "https://www.youtube.com/watch?v=ec9IQMiJBhs&t=494s\n",
    "\n",
    "- Transformers work with any kind of data\n",
    "- We represent data as a squence of vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2282415-b097-44af-af24-9431a94aa55f",
   "metadata": {},
   "source": [
    "![image.png](images/attention4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce67feb-2da7-4245-86a6-c0b40e452a8e",
   "metadata": {},
   "source": [
    "### How are they used in LLMs?\n",
    "\n",
    "- Text needs to be represented as vectors\n",
    "- So we have tokenization\n",
    "    - Take a sequence of words and deconstruct it into numerical\n",
    " \n",
    "### What is tokenization?\n",
    "\n",
    "This refers to the following lines of code here.\n",
    "\n",
    "- `from transformers import AutoTokenizer`\n",
    "- `tokenizer = AutoTokenizer.from_pretrained(\"model\")`\n",
    "\n",
    "![image.png](images/tokenization.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83852fd6-26b7-4d97-a5dc-1d1f3b63c063",
   "metadata": {},
   "source": [
    "Tokenization takes time before the training steps\n",
    "- It is independent in the pre-processing steps\n",
    "\n",
    "![image.png](images/tokenization2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e55699-7a67-4989-9b48-ec32f0e9f4ed",
   "metadata": {},
   "source": [
    "- So tokenization is transforming word into vectors which typically is obtained through word embeddings where similar words are grouped closer. Where distance between words represents similarity of two different words.\n",
    "\n",
    "- Word embedding is a simple concept. It is obtained just through how frequently different words appear in the same context under different texts\n",
    "\n",
    "Example (Notice how the word works appears repeatedly among the following phrases):\n",
    "\n",
    "- Math works every time\n",
    "- A computer works amazing\n",
    "- Transformer works great\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07753ddc-197a-4270-a2d6-75ce4a1aa2d1",
   "metadata": {},
   "source": [
    "### What are the steps in a transformer layer?\n",
    "\n",
    "The transformer consists of multiple transformer layers. Each transformer layer contains 2 items\n",
    "1. **The Feed forward neural network**\n",
    "    - 1 GeLu activation layer to double the dimension\n",
    "    - 1 GeLu activation layer to scale down the dimension\n",
    "\n",
    "But we need some way for the words to communicate per each transformer layer for each of the words. So thus we have the self attention layer\n",
    "\n",
    "2. **Attention Layer**\n",
    "    - Allows sequence of words to flow from one neighbor to the other\n",
    "    - Calculates how much representation each neighbor gets for us to compute new word\n",
    "\n",
    "**Note:** Self Attention refers calculating importance of a sequence with respect to the same sequence.\n",
    "Attention is more broad and refers to calcualting importance of a sequence with respect to overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69c18c-8e33-4c65-8198-e6c1e5882379",
   "metadata": {},
   "source": [
    "![image.png](images/transformers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3715f-3ea2-4fe8-9d4c-f51fe21f850f",
   "metadata": {},
   "source": [
    "### The Math Behind Attention Layers\n",
    "\n",
    "It is a lot of linear algebra which is depicted by a seroes of picture below.\n",
    "Please note attention scales quadratically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f4ec7-5d20-4eb0-9e15-a881494bdef7",
   "metadata": {},
   "source": [
    "- We have $W^Q$ which represents query matrix\n",
    "    - randomly intialized during training\n",
    "    - gradient descent adapts its values during backpropagation to reduce loss on training data\n",
    "    - same $W^Q$ for every value\n",
    "- We have $W^K$ which represents key matrix\n",
    "    - related to what we use as input to output\n",
    "- We also have $W^V$ which represents value matrix\n",
    "    - result of calculations, related with input\n",
    "\n",
    "**Usage in Transformers:** Three areas that we can use Q,K,V vectors\n",
    "\n",
    "1. **Encoder Self Attention**\n",
    "    - Q = K = V = Our source sentence\n",
    "2. **Decoder Self Attention**\n",
    "    - Q = K = V = Our target sentence\n",
    "3. **Decoder-Encoder attention**\n",
    "    - Q = Our target sentence\n",
    "    - K = V = Our source sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0001b14-8c2d-4dba-a375-24b2750bf9f2",
   "metadata": {},
   "source": [
    "![image.png](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883052fa-2895-4ea8-92e4-c71e1847ebcd",
   "metadata": {},
   "source": [
    "**Suppose we are calculating <u>works</u> in the image above**\n",
    "\n",
    "1. Compute scalar products of query vector of interest and every key of the word\n",
    "2. Divide the scalar product of each by the sqrt of the dimension\n",
    "3. Take the softmax of all of these values\n",
    "4. Compute the weighted sum\n",
    "\n",
    "![image.png](images/attention2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b0bc-173c-4b60-bdf2-9b3b17ca7f3a",
   "metadata": {},
   "source": [
    "Multi-head attention is used because one is simply not enough. Which gives us the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451543d1-0e90-485b-9c3f-126c7fe86c99",
   "metadata": {},
   "source": [
    "![image.png](images/attention3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c84e78-d289-4b5f-90b1-3ba16034b3aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Position Embeddings\n",
    "We also have position embeddings which is indepedent of the transformer but is associated with the position of a particular word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d79ca-b5aa-46ff-9e13-0b6f9e0ec771",
   "metadata": {},
   "source": [
    "### What about images?\n",
    "\n",
    "Images work similar but we will not go in depth with images since this is not what we are concerned about\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944312e4",
   "metadata": {},
   "source": [
    "# Checklist of Items \n",
    "- [x] What is Llama2?\n",
    "    - [x] Llama2 Tokenizer sentencepiece (https://github.com/google/sentencepiece)\n",
    "- [ ] Llama2 Documentation and Resources (https://huggingface.co/docs/transformers/main/model_doc/llama2)\n",
    "    - [x] Text Generation\n",
    "    - [ ] Text Classification\n",
    "    - [x] Optimization\n",
    "    - [x] Inference\n",
    "    - [ ] Deploy\n",
    "- [x] Prompt Engineering for LMs\n",
    "    - [x] Zero-shot prompting\n",
    "    - [x] Few-shot prompting\n",
    "    - [x] Chain of thought prompting\n",
    "    - [x] Text Prompting\n",
    "        - [x] Continuing responses from chat history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa256f-c04d-4dc8-bd33-1902b549c9bd",
   "metadata": {},
   "source": [
    "## 1) What is Llama2?\n",
    "\n",
    "Taken from: https://www.youtube.com/watch?v=yZ9jkgN2xHQ\n",
    "\n",
    "This section outlines a brief overview of what Llama2 is and how it relates to other LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4844247-f9b8-4a94-8acb-7deaa3963194",
   "metadata": {},
   "source": [
    "### Brief Overview\n",
    "An outline of exactly what Llama2 is and a basic look at its architecture\n",
    "\n",
    "**A Brief Summary:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "956c900b-2c31-4b92-9320-9d5a0b3e4252",
   "metadata": {},
   "source": [
    "**Important to understand**\n",
    "\n",
    "- **Llama-2** $\\rightarrow$ pretrained language model\n",
    "- **Llama-2 Chat** $\\rightarrow$ finetuned chatbot that has RLHF (reinforcement learning through human feedback)\n",
    "\n",
    "Similar to GPT-3 and ChatGPT\n",
    "\n",
    "**What is RLHF?**\n",
    "\n",
    "We have a group of human reviewer which selects the best answer by the chatbot which gets feed through back into the model for fine tuning\n",
    "\n",
    "**Pretraining the Model**\n",
    "\n",
    "![image.png](images/llm_basics1.png)\n",
    "\n",
    "We feed the model tokens to pretrain an architecture which gives us the right results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd09caf-4958-4f53-b16a-4e5319ae43d0",
   "metadata": {},
   "source": [
    "Now we also have the chat version which features finetuning the model so that it behaves like a normal conversation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a26b3ee0-c69d-404d-86dd-50e7a0d89867",
   "metadata": {},
   "source": [
    "![image.png](images/llm_basics2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48a926c6-dd4e-42bf-a7c5-213e149be40d",
   "metadata": {},
   "source": [
    "**Training Overview**\n",
    "\n",
    "Below outlines the approximate training of Llama2.\n",
    "\n",
    "The only difference is that the normalization is done before the feed-forward and masked multi-attention steps.\n",
    "\n",
    "![image.png](images/decoder_architecture.png)\n",
    "\n",
    "To refer to the model code, please visit this link: https://github.com/facebookresearch/llama/blob/main/llama/model.py\n",
    "\n",
    "The model consists a stack of transformer blocks. \n",
    "\n",
    "It also contains a mask so the decoder can not cheat. It has access to information after the current word so a mask would mitigate this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f93dc-990e-4543-9382-0c848f16ffd3",
   "metadata": {},
   "source": [
    "### Summary from Paper\n",
    "\n",
    "Below is a summary and brief understanding of the paper written on Llama2\n",
    "\n",
    "**Paper Summary:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495b343-a86f-4e4f-a959-9d666de761fa",
   "metadata": {},
   "source": [
    "- Llama2 is considered safer than other open sourced models\n",
    "- Use PPO (type of reinforcement learning algorithm) $\\rightarrow$ ability to score outputs (https://www.youtube.com/watch?v=5P7I-xPq8u8)\n",
    "    - **Two types of rewards:** safety & helpfulness\n",
    "    - Two rewards conflict with each other\n",
    "\n",
    "**Pretraining**\n",
    "- Used auto-regressive transformer\n",
    "    - It is able to look at its own output, it is predicting next token based on all of its token it it already predicted\n",
    "- Used grouped-query attention (GQA) (https://www.youtube.com/watch?v=pVP0bu8QA2w)\n",
    "    - Imagine we have 8 queries which 4 sub groups.\n",
    "    - Each subgroup gets one key and one value.\n",
    "- Standard transformer architecture (https://arxiv.org/pdf/1706.03762.pdf)\n",
    "    - Extremely important paper in the field of deep learning\n",
    "    - Talks about how this standard architecture of transformers is all you need\n",
    "- Prenormalization (https://arxiv.org/pdf/1910.07467.pdf)\n",
    "    - Normalization is done before attention head\n",
    "- SwiGLU activation function (https://www.ai-contentlab.com/2023/03/swishglu-activation-function.html)\n",
    "    - Combination of Swish and GLU activation functions\n",
    "    - Swish: non-motonic which leads to better optimization and faster convergence\n",
    "    - GLU: similar to Swish but linear function is gated by a sigmoid activation function\n",
    "    - SwiGLU: Swish function is used instead to gate the linear function part of GLU\n",
    "- Rotary positional embeddings\n",
    "    - Type of method to do positional embedding before decoder step\n",
    "\n",
    "**Hyperparameters**\n",
    "- AdamW optimizer, $\\beta_1 = 0.9, \\; \\beta_2 = 0.95, \\;  \\epsilon=10^{-5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9aaf75-7ada-41b3-a09f-086455c79f5a",
   "metadata": {},
   "source": [
    "## 2) Llama2 Documentation\n",
    "The below section will dive into reading and discovering how to use Llama2 based on documentation huggingface found in the link: https://huggingface.co/docs/transformers/main/en/model_doc/llama2\n",
    "\n",
    "The main topics covered will be ones below:\n",
    "- [x] Setting up Llama2 locally (https://www.youtube.com/watch?v=AOzMbitpb00)\n",
    "- [ ] Llama2 Starting Documents\n",
    "    - [x] Intro, Transformers and PEFT (https://huggingface.co/blog/llama2#fine-tuning-with-peft)\n",
    "        - [x] Using Transformers\n",
    "    - [ ] Llama 2 - Every resource you need (https://www.philschmid.de/llama-2)\n",
    "        - [ ] What is Llama2? Etc\n",
    "- [ ] Text Generation\n",
    "    - [ ] fine-tune Llama 2 in Google Colab using QLoRA and 4-bit precision\n",
    "    - [ ] fine-tune the “Llama-v2-7b-guanaco” model with 4-bit QLoRA and generate Q&A datasets from PDFs\n",
    "- [ ] Text Classification\n",
    "    - [ ] fine-tune the Llama 2 model with QLoRa, TRL, and Korean text classification dataset.\n",
    "- [ ] Optimization\n",
    "    - [ ] fine-tune Llama2 with DPO\n",
    "    - [ ] Extended guide: instruction-tune Llama2\n",
    "    - [ ] Notebook on fine-tune the Llama2 using QLoRa and TRL\n",
    "- [ ] Inference\n",
    "    - [ ] How to Quantize the Llama 2 model using GPTQ from the AutoGPTQ Library\n",
    "    - [ ] How to run the Llama 2 Chat Model with 4-bit quantization on a local computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc24899-a415-4958-8e1b-cc274c623202",
   "metadata": {},
   "source": [
    "### Setting up Llama2 locally\n",
    "\n",
    "Below we go through a series of steps to set up Llama2 on a local machine with huggingface. \n",
    "\n",
    "**Step by Step Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8072b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classic libraries needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import accelerate\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0041ca-80dc-4268-bb1b-a14bbb49c6d6",
   "metadata": {},
   "source": [
    "Now we set up the model with our token from hugging face. We set the cache_dir so the model does not need to be reloaded everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6e0bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 14.58s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "#just so that you are using CUDA to speed up training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    cache_dir = \"/data/yash/base_models\",\n",
    "    device_map = \"auto\",\n",
    "    token = \"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\"\n",
    ")\n",
    "\n",
    "#many times we do huggingface-cli login.\n",
    "#so what we do is we run terminal and then run the token!\n",
    "#this is more common practice but we will not do so time time.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                         cache_dir = \"/data/yash/base_models\",\n",
    "                                         token = \"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543cb3b3-a89f-4ff7-8d89-54bb78d916dd",
   "metadata": {},
   "source": [
    "Below is our input which returns tensors represented by our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7f66bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 2296,  338]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting text to the numbers down below so we can feed into LLMs\n",
    "inputs = tokenizer(\"She is\", return_tensors=\"pt\").to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ae202-a58e-4d04-a9e7-9e8555903a55",
   "metadata": {},
   "source": [
    "We place our these tensors inputs into our model to generate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659e8b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  2296,   338,  5279,  2323, 29889,  2296,   756, 29797,   263,\n",
       "          1353,   310,  1757]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#laptop is a bit slow training this...\n",
    "#took approximately 7 mins to train this\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "#these numbers are generated from the tokens\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab2f4f-bf93-4520-aab0-76b6e3c3d02c",
   "metadata": {},
   "source": [
    "We then decode the tensor above with the same tokenizer to get our response back into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb926864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She is currently single. She has dated a number of men'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "special tokens: adds additional tokens to help \n",
    "'''\n",
    "\n",
    "response = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb100332-1242-427d-9470-ba6f05ac85e4",
   "metadata": {},
   "source": [
    "Below, we create a function which does the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3abcb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function where we get prompts\n",
    "'''\n",
    "model.generate(temperature = ) setting.\n",
    "lower temperates leads to more deterministic outputs\n",
    "doesn't take in 0 \n",
    "\n",
    "summary: a lower temperature leads to the same generated response everytime. \n",
    "a higher temperature leads to more varied responses generated everytime.\n",
    "think of this almost like a random seed.\n",
    "'''\n",
    "\n",
    "def get_llama2_response(prompt,max_new_tokens=10):\n",
    "    inputs = tokenizer(prompt,return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs,max_new_tokens=max_new_tokens, temperature = 0.01)\n",
    "    response = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90551131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q: What is 2 + 2? A: 4\\nQ:'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "prompt = \"Q: What is 2 + 2? A:\"\n",
    "get_llama2_response(prompt,max_new_tokens=5)\n",
    "\n",
    "#the model doesn't know when to stop after answering this version of the model. \n",
    "#that is why we want to utilize the chat version so the model can know when to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e298247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#huggingface leaderboards to look at which models perform best under which settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3725a4f",
   "metadata": {},
   "source": [
    "### Difference between Llama2 and Llama2 Chat?\n",
    "Llama2 and Llama2 chat's main difference lies in the ability to formulate responses and continue conversation with the user.\n",
    "\n",
    "When we give LLMs a command or question, we expect a completion, not an answer.\n",
    "\n",
    "- So if we give it an instruction, it might give us more questions to finish the \"completition\" rather than just giving us an answer.\n",
    "- So we wish to 'chat' with our LLM. This is why we have a chat version.\n",
    "\n",
    "**Code Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb19865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 14.99s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:33<00:00, 16.68s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "#base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    cache_dir = \"/data/yash/base_models\",\n",
    "    device_map = \"auto\",\n",
    "    token = \"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\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                         cache_dir = \"/data/yash/base_models\",\n",
    "                                         token = \"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\")\n",
    "\n",
    "#chat model\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    cache_dir = \"/data/yash/base_models\",\n",
    "    device_map = \"auto\",\n",
    "    token = \"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\"\n",
    ")\n",
    "\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                         cache_dir = \"/data/yash/base_models\",\n",
    "                                         token = \"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0065559d-8985-4bb2-9aa1-c2520a2bae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "\n",
      "Comment: Of course! Based on your interest in \"Breaking Bad\" and \"Band of Brothers,\" here are some other shows you might enjoy:\n",
      "\n",
      "1. \"The Sopranos\" - This HBO series is a crime drama that explores the life of a New Jersey mob boss, Tony Soprano, as he\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model = chat_model,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    tokenizer = chat_tokenizer\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "     'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bfe9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama2_chat_response(prompt,max_new_tokens=10):\n",
    "    inputs = chat_tokenizer(prompt,return_tensors=\"pt\").to(device)\n",
    "    outputs = chat_model.generate(**inputs,max_new_tokens=max_new_tokens, temperature = 0.01)\n",
    "    response = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30ebd5bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 2 in text?\\n2 in text is the number two written in a text format. It is represented as \"2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is 2 in text?\"\n",
    "get_llama2_chat_response(prompt,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdde71-8baf-4c3f-aeb9-93f55cf24c8c",
   "metadata": {},
   "source": [
    "Notice how different from the regular version, the chat version stops after it has given the answer rather than overflooding until maximum tokens is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa9ac7",
   "metadata": {},
   "source": [
    "### Llama2-Chat Instructional Prompts\n",
    "\n",
    "We must use specific tokens since it is chat scenario. \n",
    "More can be found here.\n",
    "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "- `<<SYS>> Telling Llama2 model how you expect it to respond or answer <<\\SYS>>`\n",
    "- `[Inst] Commands or questions from user [/Inst]` from users.\n",
    "- Also need tags from users\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8d4dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_llama2_chat_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#classic instructions form for chat version\u001b[39;00m\n\u001b[0;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INST] What is 2 in text? [/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m get_llama2_chat_response(prompt,\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_llama2_chat_response' is not defined"
     ]
    }
   ],
   "source": [
    "#classic instructions form for chat version\n",
    "prompt = \"[INST] What is 2 in text? [/INST]\"\n",
    "get_llama2_chat_response(prompt,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ec565",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Using Prompt Engineering in Llama2** \n",
    "\n",
    "Prompt engineering is important to guide LLMs to give answers that is preferred.\n",
    "\n",
    "Below is the reference to the site for techniques used:\n",
    "https://www.promptingguide.ai/\n",
    "\n",
    "1. **Zero-shot Prompting** - No examples given, straightforward prompts that asks for the answer\n",
    "2. **Few-shot Prompting** - Leading examples to guide the model to respond in a certain way\n",
    "3. **Chain of Thought Prompting** - Guiding the response to give an answer that is appropriate after a series of connecting ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2976aec2-723b-4ad3-b341-5995bf3dfa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST]\n",
      "Classify the text into neutral, negative or positive.\n",
      "Text: I just love it\n",
      "[/INST]\n",
      "Classification: Positive\n"
     ]
    }
   ],
   "source": [
    "#Zero-shot prompting\n",
    "prompt = '''\n",
    "[INST]\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: I just love it\n",
    "[/INST]\n",
    "'''\n",
    "print(get_llama2_chat_response(prompt,max_new_tokens=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e9b3f-d31d-4d4f-9b7e-a41ccfe6f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //\n",
    "'''\n",
    "print(get_llama2_chat_response(prompt,max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f39cf1cb-3d02-4905-bec9-870712e40be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST] <<SYS>>\n",
      "Classify the  text into neutral, negative or positive.\n",
      "<</SYS>>\n",
      "\n",
      "I like this. [/INST]\n",
      "positive\n",
      "[INST] I hate this [/INST]\n",
      "negative\n",
      "[INST] this is ok [/INST]\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "#Few-shot prompting\n",
    "#<<SYS>> is only given once per prompt. \n",
    "#the INST is given to the LLM and the rest is figured out by itself\n",
    "#we expect the response of the LLM to be the final thing\n",
    "prompt = '''\n",
    "[INST] <<SYS>>\n",
    "Classify the  text into neutral, negative or positive.\n",
    "<</SYS>>\n",
    "\n",
    "I like this. [/INST]\n",
    "positive\n",
    "[INST] I hate this [/INST]\n",
    "negative\n",
    "[INST] this is ok [/INST]\n",
    "'''\n",
    "print(get_llama2_chat_response(prompt,max_new_tokens=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a84d4d10-82d4-4e18-8f2a-5cf3d225fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST]The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.[/INST]\n",
      "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
      "[INST]The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.[/INST]\n",
      "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
      "[INST]The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.[/INST]\n",
      "[INST]A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
      "[INST]The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.[/INST]\n",
      "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
      "[INST]The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. [/INST]\n",
      "A: Adding all the odd numbers (5, 13, 7) gives 25. The answer is True.\n"
     ]
    }
   ],
   "source": [
    "#Chain of Thought Prompting\n",
    "prompt = '''\n",
    "[INST]The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.[/INST]\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "[INST]The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.[/INST]\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "[INST]The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.[/INST]\n",
    "[INST]A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "[INST]The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.[/INST]\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "[INST]The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. [/INST]\n",
    "'''\n",
    "print(get_llama2_chat_response(prompt,max_new_tokens=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0c886-5920-4110-9af5-400b8795c645",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**SYS Instructions for Llama2**\n",
    "\n",
    "Below are examples on how to input `<<SYS>>` commands from users to control how the model should respond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ce9ce-3f8e-4fd1-803c-3d2bbe98c62a",
   "metadata": {},
   "source": [
    "**Example 1:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa0d4cf0-e11b-40cf-8126-272d1a6372ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST] <<SYS>>\n",
      "Respond only in emojis!\n",
      "<</SYS>>\n",
      "\n",
      "What is the weather like today?[/INST]\n",
      "🌞☁️🌨\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "[INST] <<SYS>>\n",
    "Respond only in emojis!\n",
    "<</SYS>>\n",
    "\n",
    "What is the weather like today?[/INST]\n",
    "'''\n",
    "print(get_llama2_chat_response(prompt,max_new_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5c697-2556-42db-9149-a558c09b39a1",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "481d1135-8bce-4741-b902-dfd6a34935a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST] <<SYS>\n",
      "Respond only by quoting from the TV series Breaking Bad\n",
      "<</SYS>>\n",
      "\n",
      "Hi[/INST]\n",
      "\"Say my name. Say it.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "[INST] <<SYS>\n",
    "Respond only by quoting from the TV series Breaking Bad\n",
    "<</SYS>>\n",
    "\n",
    "Hi[/INST]\n",
    "'''\n",
    "print(get_llama2_chat_response(prompt,max_new_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75f833-0a47-48e0-ac1f-393fe22771fd",
   "metadata": {},
   "source": [
    "**Continuing the chat**\n",
    "\n",
    "We can also perform the following to make it so that the chat can follow along conversations and already said messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c02981b-2c71-4cc9-89ce-d59dab0ced23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST] <<SYS>\n",
      "Respond only by quoting from the TV series Breaking Bad\n",
      "<</SYS>>\n",
      "\n",
      "Hi[/INST]\n",
      "Say my name. Say it.\n",
      "[INST] Where do we cook? [/INST]\n",
      "\"The cooking is in the kitchen, my friend.\" - Walter White\n"
     ]
    }
   ],
   "source": [
    "#the prompt below continues the conversation\n",
    "prompt = '''\n",
    "[INST] <<SYS>\n",
    "Respond only by quoting from the TV series Breaking Bad\n",
    "<</SYS>>\n",
    "\n",
    "Hi[/INST]\n",
    "Say my name. Say it.\n",
    "[INST] Where do we cook? [/INST]\n",
    "'''\n",
    "print(get_llama2_chat_response(prompt,max_new_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b25bc-f510-498a-8b85-e706ade62692",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "Below in this section, we will go through documents and concepts that deals with optimization portion of Llama2\n",
    "\n",
    "- [x] Finetuning Llama2 with DPO\n",
    "- [x] Instruction Tune Llama2\n",
    "- [x] Llama2 using QLoRa and TRL\n",
    "\n",
    "**Sections outlined below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f44b7e-c8f9-438f-b2b1-710eabb72ccb",
   "metadata": {},
   "source": [
    "#### 1 - Finetuning Llama2 with DPO\n",
    "\n",
    "**What is DPO? -** DPO Stands for Direct Preference Optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de48724-7dd6-495a-8190-a802fac82fe7",
   "metadata": {},
   "source": [
    "##### **A Brief Paper Summary:**\n",
    "Summary from the paper: https://www.youtube.com/watch?v=pzh2oc6shic&t=6s\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b660542c-1423-4371-89d8-fa9b8c499b2e",
   "metadata": {},
   "source": [
    "<u>Recall the classic RLHF pipeline.</u>\n",
    "\n",
    "1. **supervised fine-tuning (SFT)** - on a high-quality dataset for tasks of interests, e.g. dailogue, instruction following, summarization etc.\n",
    "2. **preference sampling** - SFT model is prompted and its answers are sent to human reviewers who express preferences for answers.\n",
    "3. **reinforcement-learning optimization** - We used the learned reward function to provide feedback to the LLM. Formulate an optimization problem and maximize using PPO.\n",
    "\n",
    "<i>**So how is DPO different from PPO?**</i>: It bypasses <u>both</u> **explicit reward estimation** and **reinforcement learning** by directly optmizing the model using preference data.\n",
    "\n",
    "<i>**Code for the Loss Function**</i>\n",
    "\n",
    "![image.png](images/dpo_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b1138-5082-4cb4-a15f-f72b7e54c199",
   "metadata": {},
   "source": [
    "##### **Diving into the Theory:** \n",
    "\n",
    "**Main Idea**: The core reward formula that can generate a loss function without the need for reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414989f2-5881-49a5-a70b-dae7a9ac2175",
   "metadata": {},
   "source": [
    "$$ L_{DPO} (\\pi_\\theta; \\pi_\\text{ref}) = \\mathbb{E}_{(x,y_w,y_l)\\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi^*(y_w|x)}{\\pi_\\text{ref}(y_w|x)} - \\frac{\\pi^*(y_l|x)}{\\pi_\\text{ref}(y_l|x)} \\right) \\right] $$\n",
    "\n",
    "- This gives a loss function that maximizes reward based on choosing the winning prompt over the losing prompt\n",
    "\n",
    "- More recent IPO model which says that DPO violates fundamental assumptions\n",
    "- $\\beta$ is a parameter that controls how large a KL distance can be travelled\n",
    "    - How different policy is before and after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca328c-4892-42e6-9771-291da8a503c9",
   "metadata": {},
   "source": [
    "##### **More Indepth Notes from Videos Below**\n",
    "Below are the list of references: \n",
    "- [x] Original Paper (https://arxiv.org/pdf/2305.18290.pdf) \n",
    "- [x] DPO vs RL (https://www.youtube.com/watch?v=YJMCSVLRUNs)\n",
    "- [x] Direct Preference Optimzation (https://www.youtube.com/watch?v=XZLc09hkMwA)\n",
    "- [ ] Video Explanation 2 (https://www.youtube.com/watch?v=HCFTXTn1PHA)\n",
    "- [ ] Video Explanation 3 (https://www.youtube.com/watch?v=E5kzAbD8D0w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac2528-c500-4c9d-9965-cf501417fb3e",
   "metadata": {},
   "source": [
    "###### **DPO v RL (Video 1)**\n",
    "DPO acts as an alernative for RLHF without any RL update rule.\n",
    "\n",
    "**Expand below to see notes:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc9c18-f499-4c71-8ab6-958fbedf9e8b",
   "metadata": {},
   "source": [
    "\n",
    "<u>Recall the formula for PPO below</u>\n",
    "$$\\pi_r (y|x) = \\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$$\n",
    "\n",
    "- so now DPO is no longer a policy model but a reward based model\n",
    "\n",
    "<u>Some formulas in context of RLHF</u>\n",
    "\n",
    "- Bradley Terry preference model- $$p^*(y_1>y_2|x) = \\frac{\\exp(r^*(x,y1)}{exp(r^*(x,y_1)) + exp(r^*(x,y_2))}$$\n",
    "    - Probability that one entry wins over another, normalized with exponentials\n",
    "- Reward model loss - $$L_R(r_\\phi, D) = - \\mathbb{E}_{(x,y_w,y_l)\\sim D}[\\log\\sigma(r_\\phi(x,y_w) - r_\\phi(x,y_l)] $$\n",
    "    - Seperating the winning denoted by $w$, and completion $y$\n",
    "- Policy objective - $$\\underset{\\pi_\\theta}{\\max} \\mathbb{E}_{x \\sim D,y \\sim \\pi_\\theta(y|x)}[r_\\phi(x,y)] - \\beta \\; \\mathbb{D}_{KL}[\\pi_\\theta (y|x) || \\pi_\\text{ref}(y|x)]$$\n",
    "    - KL constraint maximization problem, so it prevents gibberish that is being generated.\n",
    "    - A.K.A. Kullback-Leibler divergence (https://www.lesswrong.com/posts/bQ3xeaWsjzbAeStHB/distillation-rl-with-kl-penalties-is-better-viewed-as)\n",
    "        - RL with Bayesian inference\n",
    "        - More ways and intutive thinking in this article (https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6723b9-e65c-4112-b7ac-fdde34b61b88",
   "metadata": {},
   "source": [
    "**Optimal Policy**\n",
    "From the policy objective, the paper goes in depth on its derivation to get the following optimal solution:\n",
    "\n",
    "$$\\pi(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_\\text{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$$\n",
    "\n",
    "- This is obtained through the Gibbs' inequality which gives us the result of minimized KL-divergence at 0 i.f.f. two distributions are identical. '\n",
    "\n",
    "So this helps us solve for the reward of function of the form:\n",
    "\n",
    "$$ r(x,y) = \\beta \\log \\frac{\\pi_r(y|x)}{\\pi_\\text{ref}(y|x)} + \\beta \\log Z(x) $$\n",
    "\n",
    "- reward is logprob ratio\n",
    "- increases the likelihood of chosen tokens\n",
    "\n",
    "Plugging in to the Bradley Terry (with softmax conversion) \n",
    "\n",
    "$$p^*(y_1>y_2|x) = \\frac{1}{1 + \\exp \\left( \\beta \\log \\frac{\\pi^*(y_2|x)}{\\pi_\\text{ref}(y_2|x)} - \\beta \\frac{\\pi^*(y_1|x)}{\\pi_\\text{ref}(y_1|x)} \\right) }$$\n",
    "\n",
    "- This gives a way to calculate the probability that the chosen prompt is better than the rejected prompt based on the data\n",
    "\n",
    "Reformulate the above to get a loss $\\rightarrow$ get the MLE objective\n",
    "\n",
    "$$ L_{DPO} (\\pi_\\theta; \\pi_\\text{ref}) = \\mathbb{E}_{(x,y_w,y_l)\\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi^*(y_w|x)}{\\pi_\\text{ref}(y_w|x)} - \\frac{\\pi^*(y_l|x)}{\\pi_\\text{ref}(y_l|x)} \\right) \\right] $$\n",
    "\n",
    "- Above is the difference between $y_w$ given $x$ and $y_l$ given $x$.\n",
    "- Pairwise difference between the winning and losing prompts\n",
    "- Loss function is very simple in PPO compared to DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746f819-3a71-46c5-8128-b7453406a534",
   "metadata": {},
   "source": [
    "###### **Direct Preference Optimization (Video 2)**\n",
    "\n",
    "This video covers the following four sections\n",
    "- How does normal Training/Fine-Tuning work?\n",
    "- How does DPO work?\n",
    "- DPO datasets\n",
    "- Training Notebook Runthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5961a5-f467-4cb4-87ed-bfbf2a8cf2e0",
   "metadata": {},
   "source": [
    "**Google Colab Code:** https://colab.research.google.com/drive/1AP9jewCrK6uSItWeRBePbkY9EFiYiii4?usp=sharing\n",
    "\n",
    "Run and followed coding instructions to fine-tune tinyLlama using DPO. Also wrangled datasets from the website to fit instruction prompting for Llama2 chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5816dff-2e43-48be-a19d-2cc7868cd68f",
   "metadata": {},
   "source": [
    "**How does normal training/fine-tuning work?**\n",
    "\n",
    "Standard training - penalize the model based on predicted vs actual next token.\n",
    "\n",
    "<u>Example:</u>  \n",
    "Training Data:\n",
    "- The Capital of Ireland is <u> Dublin </u>\n",
    "- The Capital of Ireland is <u> Dublin </u>\n",
    "- The Capital of Ireland is <u> Cork </u>\n",
    "\n",
    "From the above, we see that the model has a preference to predict dublin as the next token. If we want to bias the model with Dublin as the response $\\rightarrow$, we feed the model with data with Dublin as the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c02f0-7836-4c27-89ed-1d26cf12c7bb",
   "metadata": {},
   "source": [
    "**What about Direct Preference Optimization?**\n",
    "\n",
    "- We are going to instead drag the prob. dist. away from one answer and towards another\n",
    "\n",
    "<u> Example: </u>\n",
    "\n",
    "- Prompt $\\rightarrow$ The Capital of Ireland is ____\n",
    "\n",
    "- Chosen Response $\\rightarrow$ Dublin\n",
    "- Rejected Response $\\rightarrow$ Cork\n",
    "\n",
    "Thus, we have a new penalization method given that the \n",
    "$$\\mathbb{P}(\\text{Dublin}) \\text{ of the model being trained} > \\mathbb{P}(\\text{Dublin}) \\text{ of the reference model}$$\n",
    "$$\\mathbb{P}(\\text{Cork}) \\text{ of the model being trained} < \\mathbb{P}(\\text{Cork}) \\text{ of the reference model}$$\n",
    "Reference model is just a copy of the model before we started the DPO process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace1c52-4e1e-4651-a2f4-ac7e33330154",
   "metadata": {},
   "source": [
    "**DPO Datasets**\n",
    "- Ultrachat\n",
    "    - A series of conversation that has been had from all sort of conversations in AI\n",
    "        - Includes both good response and bad response\n",
    "        - We form pairs and we train our model\n",
    "    - Not allowed for commercial use.\n",
    "- Helpful and Harmless\n",
    "    - This trains the model to stray away from harmful results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe5dbf-ddfa-4184-a4f6-4990ecef217d",
   "metadata": {},
   "source": [
    "**DPO vs. RLHF**\n",
    "\n",
    "Previous way involved getting totally new model and then train it to recgonize good and bad answers.\n",
    "\n",
    "So the model trained will be good on identifying good or bad answers. And then you run loop on the model to get an answer $\\rightarrow$ back-propogate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57847cd6-daff-451d-8d9b-3f5072f9e6c4",
   "metadata": {},
   "source": [
    "##### **Code Examples:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0265f-60a2-4063-ba0f-799164aecc7b",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/dpo-trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20eea8-ca6c-4de3-bdb5-fe57cef094f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "900fbbec-1c23-4d16-8168-2a2529e4c70a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2 - Instruction Tune Llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e65b0-d374-458b-bfc5-051446f9b56a",
   "metadata": {},
   "source": [
    "#### 3 - Llama2 using QLoRa and TRL (Only SVTTrainer)\n",
    "- [x] Google Collab (https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing#scrollTo=nCheb2Z9Aos8)\n",
    "- [x] Youtube Video (https://www.youtube.com/watch?v=aI8cyr-gH6M&t=6s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946ebc0-9d27-4072-8c17-27b5046398db",
   "metadata": {},
   "source": [
    "**Google Collab Code:** https://colab.research.google.com/drive/1MJ8lFC81RPjsIVtm5TBgClXOyihPc3Xj?usp=sharing\n",
    "\n",
    "Followed step by step and reached training epochs in the google collab code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf70d6-ebd6-470f-930e-347ce00d97ea",
   "metadata": {},
   "source": [
    "**Youtube Video**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746580c-0209-4af3-96a6-4bbbb3e64dcc",
   "metadata": {},
   "source": [
    "Specifies and explains the repository using LLama2 training with DPO and 4bit, QLora in the github of huggingface library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3698d-93b0-497d-b21b-57b4503deef6",
   "metadata": {},
   "source": [
    "**What is LORA?** https://www.youtube.com/watch?v=X4VvO3G6_vw\n",
    "\n",
    "- Before LLMs, data typically Pre-trained and then through a custom dataset $\\rightarrow$ finetuned\n",
    "- Deployment of this bulky LLMs are getting increasingly more difficult\n",
    "- Adapters are introduced\n",
    "\n",
    "![image.png](images/lora1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3a770-d849-4f06-a193-728ee875a07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
